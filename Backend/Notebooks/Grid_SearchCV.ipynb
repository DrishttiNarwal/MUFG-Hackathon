{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f867730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Imports\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report\n",
    "import shap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b5d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2. Load dataset\n",
    "# -----------------------------\n",
    "data = pd.read_csv(r'D:\\MUFG-Hackathon\\Backend\\Dataset\\synthetic_insurance_50k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e6162c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. Feature Engineering\n",
    "# -----------------------------\n",
    "data['Policy Start Date'] = pd.to_datetime(data['Policy Start Date'], dayfirst=True)\n",
    "data['Policy End Date'] = pd.to_datetime(data['Policy End Date'], dayfirst=True)\n",
    "data['Policy Duration'] = (data['Policy End Date'] - data['Policy Start Date']).dt.days\n",
    "data['Claim Ratio'] = data['Claim Amount (AUD)'] / data['Annual Premium (AUD)']\n",
    "data['Premium per Day'] = data['Annual Premium (AUD)'] / data['Policy Duration']\n",
    "\n",
    "bins = [20,30,40,50,60,70,80]\n",
    "labels = ['21-30','31-40','41-50','51-60','61-70','71-80']\n",
    "data['Age Group'] = pd.cut(data['Age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "data = data.drop(columns=['Policy Start Date','Policy End Date'])\n",
    "\n",
    "cat_features = ['State','Insurance Type','Claim Status','Payment Frequency','Age Group']\n",
    "for col in cat_features:\n",
    "    data[col] = data[col].astype(str).fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "235b1dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. Prepare Features and Labels\n",
    "# -----------------------------\n",
    "X = data.drop(columns=['Risk Score','Product Tier'])\n",
    "y_reg = data['Risk Score']            # For regression\n",
    "y_clf = data['Product Tier']          # For classification\n",
    "\n",
    "# Encode categorical columns\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_reg_train, y_reg_test, y_clf_train, y_clf_test = train_test_split(\n",
    "    X, y_reg, y_clf, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8794a1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params (Regression): {'depth': 6, 'iterations': 300, 'l2_leaf_reg': 7, 'learning_rate': 0.05}\n",
      "Test RMSE: 0.06955698431888786\n",
      "Cross-Validation RMSE scores: [0.06960009 0.06905668 0.06914562 0.07010732 0.06955056]\n",
      "Mean CV RMSE: 0.06949205390925293\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 5. Grid Search CV for Regression (Risk Score)\n",
    "# -----------------------------\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "import numpy as np\n",
    "\n",
    "param_grid_reg = {\n",
    "    'iterations': [300, 500],\n",
    "    'depth': [6, 8],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'l2_leaf_reg': [3, 5, 7]\n",
    "}\n",
    "\n",
    "reg_model = CatBoostRegressor(loss_function='RMSE', verbose=0, random_state=42)\n",
    "\n",
    "grid_reg = GridSearchCV(estimator=reg_model,\n",
    "                        param_grid=param_grid_reg,\n",
    "                        cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "                        scoring='neg_root_mean_squared_error',\n",
    "                        n_jobs=-1)\n",
    "\n",
    "grid_reg.fit(X_train, y_reg_train)\n",
    "\n",
    "print(\"Best Params (Regression):\", grid_reg.best_params_)\n",
    "best_reg_model = grid_reg.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_reg_pred = best_reg_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_reg_test, y_reg_pred))\n",
    "print(\"Test RMSE:\", rmse)\n",
    "\n",
    "# Cross-validation RMSE\n",
    "cv_scores = cross_val_score(best_reg_model, X, y_reg,\n",
    "                            cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "print(\"Cross-Validation RMSE scores:\", -cv_scores)\n",
    "print(\"Mean CV RMSE:\", -cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc8f5364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Params (Classification): {'depth': 6, 'iterations': 300, 'l2_leaf_reg': 3, 'learning_rate': 0.05}\n",
      "Test Accuracy: 0.6768\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Basic       0.51      0.74      0.60      2461\n",
      "        Gold       0.90      0.78      0.84      2479\n",
      "     Premium       0.79      0.84      0.81      2537\n",
      "    Standard       0.55      0.35      0.43      2523\n",
      "\n",
      "    accuracy                           0.68     10000\n",
      "   macro avg       0.69      0.68      0.67     10000\n",
      "weighted avg       0.69      0.68      0.67     10000\n",
      "\n",
      "Cross-Validation Accuracy scores: [0.6769 0.6784 0.6777 0.6811 0.6778]\n",
      "Mean CV Accuracy: 0.67838\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 6. Grid Search CV for Classification (Product Tier)\n",
    "# -----------------------------\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "param_grid_clf = {\n",
    "    'iterations': [300, 500],\n",
    "    'depth': [6, 8],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'l2_leaf_reg': [3, 5, 7]\n",
    "}\n",
    "\n",
    "clf_model = CatBoostClassifier(loss_function='MultiClass', verbose=0, random_state=42)\n",
    "\n",
    "grid_clf = GridSearchCV(estimator=clf_model,\n",
    "                        param_grid=param_grid_clf,\n",
    "                        cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "                        scoring='accuracy',\n",
    "                        n_jobs=-1)\n",
    "\n",
    "grid_clf.fit(X_train, y_clf_train)\n",
    "\n",
    "print(\"\\nBest Params (Classification):\", grid_clf.best_params_)\n",
    "best_clf_model = grid_clf.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_clf_pred = best_clf_model.predict(X_test)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_clf_test, y_clf_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_clf_test, y_clf_pred))\n",
    "\n",
    "# Cross-validation Accuracy\n",
    "cv_scores_clf = cross_val_score(best_clf_model, X, y_clf,\n",
    "                                cv=5, scoring='accuracy', n_jobs=-1)\n",
    "print(\"Cross-Validation Accuracy scores:\", cv_scores_clf)\n",
    "print(\"Mean CV Accuracy:\", cv_scores_clf.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1f3a0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier Prediction Accuracy (via Regression Mapping): 0.5422\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Basic       0.46      0.58      0.52      2461\n",
      "        Gold       0.76      0.94      0.84      2479\n",
      "     Premium       0.43      0.65      0.52      2537\n",
      "    Standard       0.00      0.00      0.00      2523\n",
      "\n",
      "    accuracy                           0.54     10000\n",
      "   macro avg       0.41      0.54      0.47     10000\n",
      "weighted avg       0.41      0.54      0.47     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drish\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\drish\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\drish\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Map Regression Predictions to Tiers (based on dataset means)\n",
    "# -----------------------------\n",
    "\n",
    "# Mean risk scores per tier from dataset\n",
    "tier_means = {\n",
    "    \"Basic\": 0.313,\n",
    "    \"Standard\": 0.313,   # practically identical mean with Basic\n",
    "    \"Premium\": 0.511,\n",
    "    \"Gold\": 0.695\n",
    "}\n",
    "\n",
    "def risk_to_tier(score):\n",
    "    # Find tier whose mean is closest to predicted score\n",
    "    closest_tier = min(tier_means, key=lambda t: abs(tier_means[t] - score))\n",
    "    return closest_tier\n",
    "\n",
    "# Use regression model predictions (already trained: best_reg_model)\n",
    "y_reg_pred_scores = best_reg_model.predict(X_test)\n",
    "\n",
    "# Convert regression predictions to tiers\n",
    "y_reg_as_tier = [risk_to_tier(s) for s in y_reg_pred_scores]\n",
    "\n",
    "# Evaluate vs true labels\n",
    "print(\"Tier Prediction Accuracy (via Regression Mapping):\",\n",
    "      accuracy_score(y_clf_test, y_reg_as_tier))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\",\n",
    "      classification_report(y_clf_test, y_reg_as_tier))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
